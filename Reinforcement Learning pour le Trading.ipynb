{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Reinforcement Learning pour le Trading - Deep Q-learning",
   "metadata": {
    "cell_id": "00000-ba4a8683-5718-4f4f-8e00-81c71a581fbb",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Pour former agent de trading, nous devons créer un environnement de marché qui fournit des informations sur les prix et autres, propose des actions liées au trading et assure le suivi du portefeuille pour récompenser l'agent en conséquence de ses actions.",
   "metadata": {
    "cell_id": "00001-46aaaf2e-0dd4-4c6a-8321-eaa453017ccf",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Comment mettre en place un environnement OpenAI pour le trading",
   "metadata": {
    "cell_id": "00002-af8ff448-0182-4736-bda2-3da28321ff59",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "L'environnement OpenAI Gym permet la conception, l'enregistrement et l'utilisation d'environnements qui adhèrent à son architecture, comme décrit dans sa [documentation](https://github.com/openai/gym/tree/master/gym/envs#how-to-create-new-environments-for-gym). Le fichier [trading_env.py](trading_env.py) présente un exemple qui illustre comment créer une classe qui implémente les méthodes `step()` et `reset()` requises.\n\nL'environnement de trading consiste en trois classes qui interagissent pour faciliter les activités de l'agent :\n\n 1. La classe `DataSource` charge une série temporelle, génère quelques features, et fournit la dernière observation à l'agent à chaque pas de temps. \n\n 2. Le `TradingSimulator` suit les positions, les transactions et les coûts, ainsi que les performances. Il met également en œuvre et enregistre les résultats d'une stratégie de référence d'achat et de conservation. \n \n 3. `TradingEnvironment` orchestre lui-même le processus. ",
   "metadata": {
    "cell_id": "00003-36790f73-f590-4e56-9250-37e4dd328b3d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Une simulation simple de trading",
   "metadata": {
    "cell_id": "00005-8156d072-119f-4090-8013-f0ee2af14e8e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Pour entraîner l'agent, nous devons mettre en place un jeu simple avec un ensemble limité d'options, un état avec un petit nombre de dimensions et d'autres paramètres qui peuvent être facilement modifiés et étendus.\n\nPlus précisément, l'environnement échantillonne une série temporelles du prix d'actions pour un seul téléscripteur en utilisant une date de début aléatoire pour simuler une période de négociation qui, par défaut, contient 252 jours, ou 1 an. L'état contient le prix et le volume (mis à l'échelle), ainsi que certains indicateurs techniques comme les rangs des percentiles du prix et du volume, un indice de force relative (Relative Strength Index, RSI), ainsi que les rendements à 5 et 21 jours. L'agent peut choisir entre trois actions :\n\n- **Acheter** : Investir du capital pour une position longue sur l'action\n- **Flat** : Ne conserver que les liquidités\n- **Vente à découvert** : Prendre une position courte égale au montant du capital.\n\nL'environnement tient compte du coût de transaction, qui est fixé à 10 points de base par défaut. Il déduit également un coût temps de 1 point de base par période. Il suit la valeur nette d'inventaire (VNA ou Net Asset Value NAV) du portefeuille de l'agent et la compare à celle du portefeuille du marché (qui négocie sans friction afin d'élever la barre pour l'agent).",
   "metadata": {
    "cell_id": "00006-d10db016-efeb-4df0-bfe7-dd3f8aea94c0",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "We use the same DDQN agent and neural network architecture that successfully learned to navigate the Lunar Lander environment. We let exploration continue for 500,000 time steps (~2,000 1yr trading periods) with linear decay of ε to 0.1 and exponential decay at a factor of 0.9999 thereafter.",
   "metadata": {
    "cell_id": "00007-11c18bdb-95e1-41c2-890d-5b9045fd75d7",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Imports & Réglages",
   "metadata": {
    "cell_id": "00008-df3c0f49-fc85-47b5-ac6a-bc4d6705398f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Imports",
   "metadata": {
    "cell_id": "00009-af79348e-42d7-4edd-ba74-e28e8f03306d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00010-3aa35dfc-2bec-42cf-83ac-cd68d9327675",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "17e3dbca",
    "execution_start": 1622459866411,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "import warnings\nwarnings.filterwarnings('ignore')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.634858Z",
     "start_time": "2021-02-25T06:20:27.942424Z"
    },
    "cell_id": "00011-334155be-90bf-4077-9fc1-ece39e0acba5",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2f733902",
    "execution_start": 1622460257704,
    "execution_millis": 24027,
    "deepnote_cell_type": "code"
   },
   "source": "!pip install gym\n%matplotlib inline\n!pip uninstall tensorflow \n!pip install -U tensorflow==2.3.0 \nfrom pathlib import Path\nfrom time import time\nfrom collections import deque\nfrom random import sample\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n\nimport gym\nfrom gym.envs.registration import register",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: gym in /root/venv/lib/python3.6/site-packages (0.18.3)\nRequirement already satisfied: scipy in /root/venv/lib/python3.6/site-packages (from gym) (1.4.1)\nRequirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /root/venv/lib/python3.6/site-packages (from gym) (1.6.0)\nRequirement already satisfied: numpy>=1.10.4 in /root/venv/lib/python3.6/site-packages (from gym) (1.18.5)\nRequirement already satisfied: pyglet<=1.5.15,>=1.4.0 in /root/venv/lib/python3.6/site-packages (from gym) (1.5.15)\nRequirement already satisfied: Pillow<=8.2.0 in /shared-libs/python3.6/py/lib/python3.6/site-packages (from gym) (8.2.0)\nFound existing installation: tensorflow 2.5.0\nNot uninstalling tensorflow at /shared-libs/python3.6/py/lib/python3.6/site-packages, outside environment /root/venv\nCan't uninstall 'tensorflow'. No files were found to uninstall.\nCollecting tensorflow==2.3.0\n  Using cached tensorflow-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4 MB)\nRequirement already satisfied: scipy==1.4.1 in /root/venv/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.4.1)\nRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.12.1)\nRequirement already satisfied: wheel>=0.26 in /root/venv/lib/python3.6/site-packages (from tensorflow==2.3.0) (0.36.2)\nRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (0.11.0)\nRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (0.2.0)\nRequirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (0.3.3)\nRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (3.14.0)\nRequirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (2.4.1)\nRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.32.0)\nRequirement already satisfied: six>=1.12.0 in /shared-libs/python3.6/py-core/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.16.0)\nRequirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /root/venv/lib/python3.6/site-packages (from tensorflow==2.3.0) (2.3.0)\nRequirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.1.2)\nRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.6.3)\nRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (2.10.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (3.3.0)\nRequirement already satisfied: numpy<1.19.0,>=1.16.0 in /root/venv/lib/python3.6/site-packages (from tensorflow==2.3.0) (1.18.5)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.3.0) (1.1.0)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.24.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.25.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.0.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.3)\nRequirement already satisfied: setuptools>=41.0.0 in /root/venv/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (57.0.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.2)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.7)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\nRequirement already satisfied: importlib-metadata in /shared-libs/python3.6/py-core/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\nRequirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.6)\nRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.26.2)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.0)\nRequirement already satisfied: zipp>=0.5 in /shared-libs/python3.6/py-core/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.4.1)\nRequirement already satisfied: typing-extensions>=3.6.4 in /shared-libs/python3.6/py-core/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.10.0.0)\nInstalling collected packages: tensorflow\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.5.0\n    Not uninstalling tensorflow at /shared-libs/python3.6/py/lib/python3.6/site-packages, outside environment /root/venv\n    Can't uninstall 'tensorflow'. No files were found to uninstall.\nSuccessfully installed tensorflow-2.3.0\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Settings",
   "metadata": {
    "cell_id": "00012-8986fea1-0980-4706-8595-d89e9475d60c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.638316Z",
     "start_time": "2021-02-25T06:20:29.636097Z"
    },
    "cell_id": "00013-285515d9-9cb2-495e-a381-f2b5a2dfb1cb",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8905d08a",
    "execution_start": 1622460292778,
    "execution_millis": 4,
    "deepnote_cell_type": "code"
   },
   "source": "np.random.seed(42)\ntf.random.set_seed(42)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.651863Z",
     "start_time": "2021-02-25T06:20:29.639725Z"
    },
    "cell_id": "00014-bb00d126-0203-41c6-b721-0704547066fd",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e990413e",
    "execution_start": 1622460293431,
    "execution_millis": 5,
    "deepnote_cell_type": "code"
   },
   "source": "sns.set_style('whitegrid')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.660560Z",
     "start_time": "2021-02-25T06:20:29.653287Z"
    },
    "cell_id": "00015-28c1054f-ac02-4ff3-9edf-072370628565",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d6113e12",
    "execution_start": 1622460354962,
    "execution_millis": 8,
    "deepnote_cell_type": "code"
   },
   "source": "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\nprint(gpu_devices)\nif gpu_devices:\n    print('Using GPU')\n    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\nelse:\n    print('Using CPU')",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "[]\nUsing CPU\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.672047Z",
     "start_time": "2021-02-25T06:20:29.661727Z"
    },
    "cell_id": "00016-f3449e20-6346-4a6c-b6d0-80429bfd956c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b4569734",
    "execution_start": 1622460356655,
    "execution_millis": 3,
    "deepnote_cell_type": "code"
   },
   "source": "results_path = Path('results', 'trading_bot')\nif not results_path.exists():\n    results_path.mkdir(parents=True)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Fonctions auxiliaires",
   "metadata": {
    "cell_id": "00017-3c5e44e6-2c81-47ca-8f89-1cb0cf706984",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.680256Z",
     "start_time": "2021-02-25T06:20:29.673050Z"
    },
    "cell_id": "00018-c0f843f9-335f-4ebd-a48a-51b4d4b72ce3",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1b66e3cb",
    "execution_start": 1622460358077,
    "execution_millis": 4,
    "deepnote_cell_type": "code"
   },
   "source": "def format_time(t):\n    m_, s = divmod(t, 60)\n    h, m = divmod(m_, 60)\n    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Configurer l'environnement",
   "metadata": {
    "cell_id": "00019-879f6e9b-a530-44c4-933b-1ce9ad64c7ee",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Avant d'utiliser notre environnement, on l'enregistre grâce à la méthode register.",
   "metadata": {
    "cell_id": "00020-fadf5acf-2f47-4569-9edc-a2cc40b9163c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.688161Z",
     "start_time": "2021-02-25T06:20:29.681742Z"
    },
    "cell_id": "00021-c264e796-3cfa-40c5-aedf-a93d8d76ceda",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4dfef59e",
    "execution_start": 1622460359650,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "source": "trading_days = 252",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:29.696397Z",
     "start_time": "2021-02-25T06:20:29.690404Z"
    },
    "cell_id": "00022-0d472174-eff6-4a76-a876-6133a375220c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a820d9c1",
    "execution_start": 1622460360011,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "register(\n    id='trading-v0',\n    entry_point='trading_env:TradingEnvironment',\n    max_episode_steps=trading_days\n)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Initialisation de notre environnement de Trading",
   "metadata": {
    "cell_id": "00023-d24ec196-c51c-484d-9ab5-dadd2ac54f14",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "We can instantiate the environment by using the desired trading costs and ticker:",
   "metadata": {
    "cell_id": "00024-a7fbd1af-dee9-47e2-a6ad-9e919c407f37",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00025-dd97cc70-9e97-4605-8a02-f58e58d06bdf",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ada3a77d",
    "execution_start": 1622460361500,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "source": "trading_cost_bps = 1e-3\ntime_cost_bps = 1e-4",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00026-84fdd00c-b9e2-4a19-b8c8-786b7c72eb6c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "88ee23a3",
    "execution_start": 1622460361890,
    "execution_millis": 7,
    "deepnote_cell_type": "code"
   },
   "source": "f'Trading costs: {trading_cost_bps:.2%} | Time costs: {time_cost_bps:.2%}'",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 24,
     "data": {
      "text/plain": "'Trading costs: 0.10% | Time costs: 0.01%'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00027-e26dc6f8-1bc0-42ac-bf78-ecf20bcd3187",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3999fd",
    "execution_start": 1622460362655,
    "execution_millis": 2066,
    "is_output_hidden": true,
    "deepnote_cell_type": "code"
   },
   "source": "!pip install -r requirements.txt \n\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\r\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "pwd\n\ntar -xzf ta-lib-0.4.0-src.tar.gz\n\ncd ta-lib/\n\npwd\n\n./configure --prefix=/usr\n\nmake\n\nsudo make install",
   "metadata": {
    "tags": [],
    "cell_id": "00027-bee64847-75ae-49e3-8f0b-1ac922e2f22a",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "49f5e890",
    "execution_start": 1622452858193,
    "execution_millis": 2023,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00028-0424a8ac-ee55-4ee9-ac82-2f1392e1be26",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "42b6a281",
    "execution_start": 1622464391381,
    "execution_millis": 47739,
    "deepnote_cell_type": "code"
   },
   "source": "!pip install ta-lib",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting ta-lib\n  Using cached TA-Lib-0.4.20.tar.gz (266 kB)\nRequirement already satisfied: numpy in /root/venv/lib/python3.6/site-packages (from ta-lib) (1.18.5)\nBuilding wheels for collected packages: ta-lib\n  Building wheel for ta-lib (setup.py) ... \u001b[?25l/",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\bdone\n\u001b[?25h  Created wheel for ta-lib: filename=TA_Lib-0.4.20-cp36-cp36m-linux_x86_64.whl size=1438945 sha256=021925dc5593e6c9d14308c842703654d79cfb8ec72254b1e726a835d0624e53\n  Stored in directory: /root/.cache/pip/wheels/4a/78/6a/bb3c86ccc471d0914efdc6ab3715f7d00c343392a316bf606a\nSuccessfully built ta-lib\nInstalling collected packages: ta-lib\nSuccessfully installed ta-lib-0.4.20\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00029-ea9df21c-bcd5-4cda-9413-7f87ebe63bed",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "34b70a0f",
    "execution_start": 1622464571598,
    "execution_millis": 4065,
    "deepnote_cell_type": "code"
   },
   "source": "!pip install --upgrade tables",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: tables in /root/venv/lib/python3.6/site-packages (3.6.1)\nRequirement already satisfied: numpy>=1.9.3 in /root/venv/lib/python3.6/site-packages (from tables) (1.18.5)\nRequirement already satisfied: numexpr>=2.6.2 in /root/venv/lib/python3.6/site-packages (from tables) (2.7.3)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.544485Z",
     "start_time": "2021-02-25T06:20:29.698083Z"
    },
    "cell_id": "00027-cb909540-3ac5-4621-96e3-38bb449fba35",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "99bd4c3a",
    "execution_start": 1622464575665,
    "execution_millis": 4008,
    "deepnote_cell_type": "code"
   },
   "source": "trading_environment = gym.make('trading-v0')\ntrading_environment.env.trading_days = trading_days\ntrading_environment.env.trading_cost_bps = trading_cost_bps\ntrading_environment.env.time_cost_bps = time_cost_bps\ntrading_environment.env.ticker = 'AAPL'\ntrading_environment.seed(42)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "INFO:trading_env:loading data for AAPL...\nINFO:trading_env:got data for AAPL...\nINFO:trading_env:None\n<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 9367 entries, (Timestamp('1981-01-30 00:00:00'), 'AAPL') to (Timestamp('2018-03-27 00:00:00'), 'AAPL')\nData columns (total 10 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   returns  9367 non-null   float64\n 1   ret_2    9367 non-null   float64\n 2   ret_5    9367 non-null   float64\n 3   ret_10   9367 non-null   float64\n 4   ret_21   9367 non-null   float64\n 5   rsi      9367 non-null   float64\n 6   macd     9367 non-null   float64\n 7   atr      9367 non-null   float64\n 8   stoch    9367 non-null   float64\n 9   ultosc   9367 non-null   float64\ndtypes: float64(10)\nmemory usage: 1.6+ MB\n",
     "output_type": "stream"
    },
    {
     "output_type": "execute_result",
     "execution_count": 31,
     "data": {
      "text/plain": "[42]"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Get Environment Params",
   "metadata": {
    "cell_id": "00028-7c1ab549-e8b3-45db-b217-d4af7e8a31a9",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.548145Z",
     "start_time": "2021-02-25T06:20:32.545830Z"
    },
    "cell_id": "00029-aa122207-fab0-49f5-8a19-f5d0648add6c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c219893",
    "execution_start": 1622464580495,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "state_dim = trading_environment.observation_space.shape[0]\nnum_actions = trading_environment.action_space.n\nmax_episode_steps = trading_environment.spec.max_episode_steps",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Définir l'agent de Trading ",
   "metadata": {
    "cell_id": "00030-9c0a5fa2-016e-4136-969a-13e1ca9c827e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.563692Z",
     "start_time": "2021-02-25T06:20:32.549782Z"
    },
    "cell_id": "00031-413a6cfa-89ce-45e7-894e-25e3191c2ec7",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "24fec5b8",
    "execution_start": 1622464582213,
    "execution_millis": 17,
    "deepnote_cell_type": "code"
   },
   "source": "class DDQNAgent:\n    def __init__(self, state_dim,\n                 num_actions,\n                 learning_rate,\n                 gamma,\n                 epsilon_start,\n                 epsilon_end,\n                 epsilon_decay_steps,\n                 epsilon_exponential_decay,\n                 replay_capacity,\n                 architecture,\n                 l2_reg,\n                 tau,\n                 batch_size):\n\n        self.state_dim = state_dim\n        self.num_actions = num_actions\n        self.experience = deque([], maxlen=replay_capacity)\n        self.learning_rate = learning_rate\n        self.gamma = gamma\n        self.architecture = architecture\n        self.l2_reg = l2_reg\n\n        self.online_network = self.build_model()\n        self.target_network = self.build_model(trainable=False)\n        self.update_target()\n\n        self.epsilon = epsilon_start\n        self.epsilon_decay_steps = epsilon_decay_steps\n        self.epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps\n        self.epsilon_exponential_decay = epsilon_exponential_decay\n        self.epsilon_history = []\n\n        self.total_steps = self.train_steps = 0\n        self.episodes = self.episode_length = self.train_episodes = 0\n        self.steps_per_episode = []\n        self.episode_reward = 0\n        self.rewards_history = []\n\n        self.batch_size = batch_size\n        self.tau = tau\n        self.losses = []\n        self.idx = tf.range(batch_size)\n        self.train = True\n\n    def build_model(self, trainable=True):\n        layers = []\n        n = len(self.architecture)\n        for i, units in enumerate(self.architecture, 1):\n            layers.append(Dense(units=units,\n                                input_dim=self.state_dim if i == 1 else None,\n                                activation='relu',\n                                kernel_regularizer=l2(self.l2_reg),\n                                name=f'Dense_{i}',\n                                trainable=trainable))\n        layers.append(Dropout(.1))\n        layers.append(Dense(units=self.num_actions,\n                            trainable=trainable,\n                            name='Output'))\n        model = Sequential(layers)\n        model.compile(loss='mean_squared_error',\n                      optimizer=Adam(lr=self.learning_rate))\n        return model\n\n    def update_target(self):\n        self.target_network.set_weights(self.online_network.get_weights())\n\n    def epsilon_greedy_policy(self, state):\n        self.total_steps += 1\n        if np.random.rand() <= self.epsilon:\n            return np.random.choice(self.num_actions)\n        q = self.online_network.predict(state)\n        return np.argmax(q, axis=1).squeeze()\n\n    def memorize_transition(self, s, a, r, s_prime, not_done):\n        if not_done:\n            self.episode_reward += r\n            self.episode_length += 1\n        else:\n            if self.train:\n                if self.episodes < self.epsilon_decay_steps:\n                    self.epsilon -= self.epsilon_decay\n                else:\n                    self.epsilon *= self.epsilon_exponential_decay\n\n            self.episodes += 1\n            self.rewards_history.append(self.episode_reward)\n            self.steps_per_episode.append(self.episode_length)\n            self.episode_reward, self.episode_length = 0, 0\n\n        self.experience.append((s, a, r, s_prime, not_done))\n\n    def experience_replay(self):\n        if self.batch_size > len(self.experience):\n            return\n        minibatch = map(np.array, zip(*sample(self.experience, self.batch_size)))\n        states, actions, rewards, next_states, not_done = minibatch\n\n        next_q_values = self.online_network.predict_on_batch(next_states)\n        best_actions = tf.argmax(next_q_values, axis=1)\n\n        next_q_values_target = self.target_network.predict_on_batch(next_states)\n        target_q_values = tf.gather_nd(next_q_values_target,\n                                       tf.stack((self.idx, tf.cast(best_actions, tf.int32)), axis=1))\n\n        targets = rewards + not_done * self.gamma * target_q_values\n\n        q_values = self.online_network.predict_on_batch(states)\n        q_values[[self.idx, actions]] = targets\n\n        loss = self.online_network.train_on_batch(x=states, y=q_values)\n        self.losses.append(loss)\n\n        if self.total_steps % self.tau == 0:\n            self.update_target()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Définir les hyperparamètres",
   "metadata": {
    "cell_id": "00032-53df1df6-56e4-4836-bd1a-084778a90a0f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.575368Z",
     "start_time": "2021-02-25T06:20:32.565067Z"
    },
    "cell_id": "00033-f9fb8612-cbf6-4f19-8154-ce43afd10522",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "86648867",
    "execution_start": 1622464584687,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "gamma = .99,  # discount factor\ntau = 100  # target network update frequency",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Architecture NN",
   "metadata": {
    "cell_id": "00034-6d9ce176-80fd-4650-9775-0b5365f82c66",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.584925Z",
     "start_time": "2021-02-25T06:20:32.576469Z"
    },
    "cell_id": "00035-f42a827c-86c3-4996-8a12-02ac93cca185",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "814d50df",
    "execution_start": 1622464585628,
    "execution_millis": 6,
    "deepnote_cell_type": "code"
   },
   "source": "architecture = (256, 256)  # units per layer\nlearning_rate = 0.0001  # learning rate\nl2_reg = 1e-6  # L2 regularization",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Replay d'expérience",
   "metadata": {
    "cell_id": "00036-c0e938b3-9759-4189-81cb-fe87625fef4b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.593134Z",
     "start_time": "2021-02-25T06:20:32.586645Z"
    },
    "cell_id": "00037-f4a38976-0b02-49f3-855b-c43b6d498708",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e44be96b",
    "execution_start": 1622464591385,
    "execution_millis": 3,
    "deepnote_cell_type": "code"
   },
   "source": "replay_capacity = int(1e6)\nbatch_size = 4096",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Stratégie $\\epsilon$-greedy ",
   "metadata": {
    "cell_id": "00038-a132ae81-f1fe-4730-a146-4d25b0e23868",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.603464Z",
     "start_time": "2021-02-25T06:20:32.594606Z"
    },
    "cell_id": "00039-d4e62538-5b0e-4dad-a0fd-fe1db4ce16a4",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e06df15f",
    "execution_start": 1622464592255,
    "execution_millis": 6,
    "deepnote_cell_type": "code"
   },
   "source": "epsilon_start = 1.0\nepsilon_end = .01\nepsilon_decay_steps = 250\nepsilon_exponential_decay = .99",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Créer notre agent DDQN ",
   "metadata": {
    "cell_id": "00040-4fc4ec01-114a-4151-bdbf-649892a7b26b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "We will use [TensorFlow](https://www.tensorflow.org/) to create our Double Deep Q-Network .",
   "metadata": {
    "cell_id": "00041-fc1ffa57-6bda-42a5-8a98-061b13722b9e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.613239Z",
     "start_time": "2021-02-25T06:20:32.604766Z"
    },
    "cell_id": "00042-26837648-bcbb-4ca8-8cfc-d5fdaac28d69",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "eeace099",
    "execution_start": 1622464593501,
    "execution_millis": 29,
    "deepnote_cell_type": "code"
   },
   "source": "tf.keras.backend.clear_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.720879Z",
     "start_time": "2021-02-25T06:20:32.614703Z"
    },
    "cell_id": "00043-4701f640-0652-4662-8332-d1f02aa0805f",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6a82e59a",
    "execution_start": 1622464594492,
    "execution_millis": 118,
    "deepnote_cell_type": "code"
   },
   "source": "ddqn = DDQNAgent(state_dim=state_dim,\n                 num_actions=num_actions,\n                 learning_rate=learning_rate,\n                 gamma=gamma,\n                 epsilon_start=epsilon_start,\n                 epsilon_end=epsilon_end,\n                 epsilon_decay_steps=epsilon_decay_steps,\n                 epsilon_exponential_decay=epsilon_exponential_decay,\n                 replay_capacity=replay_capacity,\n                 architecture=architecture,\n                 l2_reg=l2_reg,\n                 tau=tau,\n                 batch_size=batch_size)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.725896Z",
     "start_time": "2021-02-25T06:20:32.722143Z"
    },
    "cell_id": "00044-1f0d6655-42ee-4dfc-9295-68dda2f19a27",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3dabe60b",
    "execution_start": 1622464596346,
    "execution_millis": 9,
    "deepnote_cell_type": "code"
   },
   "source": "ddqn.online_network.summary()",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nDense_1 (Dense)              (None, 256)               2816      \n_________________________________________________________________\nDense_2 (Dense)              (None, 256)               65792     \n_________________________________________________________________\ndropout (Dropout)            (None, 256)               0         \n_________________________________________________________________\nOutput (Dense)               (None, 3)                 771       \n=================================================================\nTotal params: 69,379\nTrainable params: 69,379\nNon-trainable params: 0\n_________________________________________________________________\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Réalisation de notre expérimentation",
   "metadata": {
    "cell_id": "00045-0c07329d-5be3-4582-83e2-3bd86b18a1c1",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### Affecter les paramètres\n",
   "metadata": {
    "cell_id": "00046-70deaec3-90ea-46a5-92f5-155913da91a5",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.733088Z",
     "start_time": "2021-02-25T06:20:32.727071Z"
    },
    "cell_id": "00047-27757e70-c05c-4b1f-b12e-9534c04e22a7",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f3b28e76",
    "execution_start": 1622464599686,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "total_steps = 0\nmax_episodes = 1000",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Initialiser les variables",
   "metadata": {
    "cell_id": "00048-2d74efb6-c536-410f-bec2-ae630d84a64c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.741126Z",
     "start_time": "2021-02-25T06:20:32.734309Z"
    },
    "cell_id": "00049-bc529b33-7225-4394-9020-fbc4c8d7a501",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2086d277",
    "execution_start": 1622464601193,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "episode_time, navs, market_navs, diffs, episode_eps = [], [], [], [], []",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Fonction de visualisation",
   "metadata": {
    "cell_id": "00050-c82a90d5-26fa-43d2-8a6f-5936c8f94888",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T06:20:32.752721Z",
     "start_time": "2021-02-25T06:20:32.742471Z"
    },
    "cell_id": "00051-7062b535-2a5e-4686-936b-ccbbf57ef00e",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "22d14799",
    "execution_start": 1622464602545,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "def track_results(episode, nav_ma_100, nav_ma_10,\n                  market_nav_100, market_nav_10,\n                  win_ratio, total, epsilon):\n    time_ma = np.mean([episode_time[-100:]])\n    T = np.sum(episode_time)\n    \n    template = '{:>4d} | {} | Agent: {:>6.1%} ({:>6.1%}) | '\n    template += 'Market: {:>6.1%} ({:>6.1%}) | '\n    template += 'Wins: {:>5.1%} | eps: {:>6.3f}'\n    print(template.format(episode, format_time(total), \n                          nav_ma_100-1, nav_ma_10-1, \n                          market_nav_100-1, market_nav_10-1, \n                          win_ratio, epsilon))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Entraînement de l'agent",
   "metadata": {
    "cell_id": "00052-34828025-1be8-4957-bb94-85c8b9df2c05",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-25T06:20:28.016Z"
    },
    "cell_id": "00053-5b37a2cc-62e0-44c7-83db-eebe9738dc90",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "17990d7d",
    "execution_start": 1622464605851,
    "execution_millis": 1447321,
    "deepnote_cell_type": "code"
   },
   "source": "start = time()\nresults = []\nfor episode in range(1, max_episodes + 1):\n    this_state = trading_environment.reset()\n    for episode_step in range(max_episode_steps):\n        action = ddqn.epsilon_greedy_policy(this_state.reshape(-1, state_dim))\n        next_state, reward, done, _ = trading_environment.step(action)\n    \n        ddqn.memorize_transition(this_state, \n                                 action, \n                                 reward, \n                                 next_state, \n                                 0.0 if done else 1.0)\n        if ddqn.train:\n            ddqn.experience_replay()\n        if done:\n            break\n        this_state = next_state\n\n    # get DataFrame with seqence of actions, returns and nav values\n    result = trading_environment.env.simulator.result()\n    \n    # get results of last step\n    final = result.iloc[-1]\n\n    # apply return (net of cost) of last action to last starting nav \n    nav = final.nav * (1 + final.strategy_return)\n    navs.append(nav)\n\n    # market nav \n    market_nav = final.market_nav\n    market_navs.append(market_nav)\n\n    # track difference between agent an market NAV results\n    diff = nav - market_nav\n    diffs.append(diff)\n    \n    if episode % 10 == 0:\n        track_results(episode, \n                      # show mov. average results for 100 (10) periods\n                      np.mean(navs[-100:]), \n                      np.mean(navs[-10:]), \n                      np.mean(market_navs[-100:]), \n                      np.mean(market_navs[-10:]), \n                      # share of agent wins, defined as higher ending nav\n                      np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100), \n                      time() - start, ddqn.epsilon)\n    if len(diffs) > 25 and all([r > 0 for r in diffs[-25:]]):\n        print(result.tail())\n        break\n\ntrading_environment.close()",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "  10 | 00:00:02 | Agent: -39.3% (-39.3%) | Market:   5.6% (  5.6%) | Wins: 20.0% | eps:  0.960\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "  90 | 01:03:10 | Agent: -20.3% (-21.6%) | Market:  24.4% ( -2.1%) | Wins: 23.3% | eps:  0.644\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": " 130 | 01:39:42 | Agent: -11.4% (-27.1%) | Market:  36.4% ( 80.3%) | Wins: 26.0% | eps:  0.485\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": " 150 | 01:58:21 | Agent:  -8.2% (  5.2%) | Market:  37.9% ( 41.0%) | Wins: 26.0% | eps:  0.406\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": " 170 | 02:16:38 | Agent:  -4.0% (  6.7%) | Market:  32.7% ( 37.8%) | Wins: 28.0% | eps:  0.327\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": " 220 | 03:07:23 | Agent:   2.4% ( 31.8%) | Market:  41.2% ( 41.3%) | Wins: 33.0% | eps:  0.129\n 230 | 03:17:07 | Agent:   8.0% ( 29.2%) | Market:  35.5% ( 24.2%) | Wins: 37.0% | eps:  0.089\n 240 | 03:27:13 | Agent:  11.8% ( 47.7%) | Market:  32.7% (-14.7%) | Wins: 41.0% | eps:  0.050\n 250 | 03:42:22 | Agent:  11.5% (  1.9%) | Market:  31.5% ( 28.6%) | Wins: 42.0% | eps:  0.010\n 260 | 03:56:34 | Agent:  15.7% ( 44.6%) | Market:  33.1% ( 31.6%) | Wins: 46.0% | eps:  0.009\n 270 | 04:10:58 | Agent:  17.3% ( 22.6%) | Market:  31.5% ( 21.8%) | Wins: 46.0% | eps:  0.008\n 280 | 04:25:28 | Agent:  20.6% ( 29.2%) | Market:  33.6% ( 84.3%) | Wins: 45.0% | eps:  0.007\n 290 | 04:40:05 | Agent:  21.0% ( -1.6%) | Market:  28.4% (  9.6%) | Wins: 46.0% | eps:  0.007\n 300 | 04:54:53 | Agent:  22.9% ( 30.2%) | Market:  31.0% ( 46.1%) | Wins: 45.0% | eps:  0.006\n 310 | 05:09:40 | Agent:  26.4% ( 28.6%) | Market:  31.0% ( 36.7%) | Wins: 46.0% | eps:  0.005\n",
     "output_type": "stream"
    },
    {
     "output_type": "error",
     "ename": "KernelInterrupted",
     "evalue": "Execution interrupted by the Jupyter kernel.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=0f8ffaa5-0b99-4437-a638-6f87b9da36c0' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.906px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "deepnote_notebook_id": "6e536eeb-762a-4dab-a273-7e50f0ec4145",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}